<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Monica Alexander on Monica Alexander</title>
    <link>/</link>
    <description>Recent content in Monica Alexander on Monica Alexander</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Monica Alexander</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Assessment of Changes in the Geographical Distribution of Opioid-Related Mortality Across the United States by Opioid Type, 1999-2016</title>
      <link>/publication/opioid_geographic/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/opioid_geographic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The concentration and uniqueness of baby names in Australia and the US</title>
      <link>/2019/01/21/the-concentration-and-uniqueness-of-baby-names-in-australia-and-the-us/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/21/the-concentration-and-uniqueness-of-baby-names-in-australia-and-the-us/</guid>
      <description>


&lt;p&gt;Some great people have compiled historical data on baby names into R packages for both the US &lt;a href=&#34;https://github.com/hadley/babynames&#34;&gt;(thanks to Hadley Wickham)&lt;/a&gt; and Australia &lt;a href=&#34;https://github.com/ropenscilabs/ozbabynames&#34;&gt;(thanks to the Monash group)&lt;/a&gt;. This makes answering all manner of baby-name-related questions easy.&lt;/p&gt;
&lt;p&gt;I was interested in looking at the distribution of baby names in these populations over time — that is, how concentrated are name choices in the most popular baby names? Is there a big difference between the number of babies that are called the most popular names compared to other names, or is the distribution more evenly spread?&lt;/p&gt;
&lt;p&gt;The summary: names are very concentrated — the majority of babies are called a name from a relatively small subset. However, baby name concentration is declining over time, and additionally, the number of unique names is increasing.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;p&gt;I used the used the &lt;code&gt;babynames&lt;/code&gt; and &lt;code&gt;ozbabynames&lt;/code&gt; packages to look at names in the US and Australia. You will need to install the Australian version from &lt;a href=&#34;https://github.com/ropenscilabs/ozbabynames&#34;&gt;GitHub&lt;/a&gt;. I restricted the period to be 1960-2015 where both datasets had data. For the Australian baby names, I restricted the dataset to only include South Australia, Western Australia and New South Wales, as the other states did not have full coverage over the specified time period.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Each dataset gives us the name, sex, year and count of number of babies. The following code loads them in and creates one tibble with both countries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load in the packages required
library(ozbabynames)
library(babynames)
library(tidyverse)
library(reldist) 

# get the Australian and US data in one big tibble

da &amp;lt;- ozbabynames %&amp;gt;% 
  filter(state %in% c(&amp;quot;New South Wales&amp;quot;, &amp;quot;South Australia&amp;quot;, &amp;quot;Western Australia&amp;quot;),
         year&amp;gt;1959, year&amp;lt;2016) %&amp;gt;% 
  mutate(sex = ifelse(sex==&amp;quot;Female&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;M&amp;quot;)) %&amp;gt;% 
  group_by(sex, year, name) %&amp;gt;% 
  summarise(count = sum(count)) %&amp;gt;% 
  arrange(sex, year, count) %&amp;gt;% 
  mutate(country = &amp;quot;AUS&amp;quot;) %&amp;gt;% 
  filter(count&amp;gt;4) # remove weird stuff with really low counts

du &amp;lt;- babynames %&amp;gt;% 
  mutate(country = &amp;quot;USA&amp;quot;) %&amp;gt;% 
  rename(count = n) %&amp;gt;% 
  arrange(sex, year, count) %&amp;gt;% 
  filter(count&amp;gt;4, year&amp;gt;1959, year&amp;lt;2016) %&amp;gt;% 
  select(-prop)

db &amp;lt;- da %&amp;gt;% 
  bind_rows(du)

head(db)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
## # Groups:   sex, year [1]
##   sex    year name    count country
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  
## 1 F      1960 Alana       5 AUS    
## 2 F      1960 Alice       5 AUS    
## 3 F      1960 Antonia     5 AUS    
## 4 F      1960 Beth        5 AUS    
## 5 F      1960 Briony      5 AUS    
## 6 F      1960 Bronwen     5 AUS&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the US is much larger than Australia — there are around 60 times more babies in the US dataset. For example, in 2015 there were 3.7 million births in the US, compared to around 57,000 in Australia. This means the trends and patterns will be noisier for Australia.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db %&amp;gt;% 
  group_by(year, country) %&amp;gt;% 
  summarise(n = sum(count)) %&amp;gt;%
  filter(year==2015)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
## # Groups:   year [1]
##    year country       n
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1  2015 AUS       56810
## 2  2015 USA     3668183&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;baby-names-are-concentrated-in-a-small-subset-of-names&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Baby names are concentrated in a small subset of names&lt;/h1&gt;
&lt;p&gt;To look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.&lt;/p&gt;
&lt;p&gt;The plot below shows the Gini coefficients by country and sex for the period 1960-2015. We can see that, in general, the Gini coefficients are high, meaning that most babies have similar names. Concentration of names is higher in the US compared to Australia and coefficients are generally decreasing over time, particularly for the US. In the US, concentration of names is higher for boys, while in Australia, the sex difference is less clear.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db %&amp;gt;% 
  group_by(country, sex, year) %&amp;gt;% 
  summarise(gini = gini(count)) %&amp;gt;% 
  ggplot(aes(year, gini, color = sex, lty = country)) + 
  geom_line(lwd = 1.1) +
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;) +
  ggtitle(&amp;quot;Gini coefficients for baby names \nAustralia and USA, 1960-2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-20-01-babynames_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can plot this concentration a different way: let’s look at the proportion of babies who have a name in the top 5% most popular names.&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that the trends and patterns are pretty much identical to those above. The levels are quite high: in 1960 in the US, almost 90% of all babies born were called a name that was in the top 5% most popular names (note that this corresponds to the around the 250 most popular names).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db %&amp;gt;% 
  group_by(sex, year, country) %&amp;gt;% 
  mutate(id = row_number()-1,
         cumul_count = cumsum(count)/max(cumsum(count))) %&amp;gt;% # get cumulative proportion of babies with each name
  mutate(rank = ntile(id, 20)) %&amp;gt;%  # find the top 5th percentile
  filter(rank==20) %&amp;gt;% 
  slice(1) %&amp;gt;% 
  ggplot(aes(year, 1-cumul_count, color = sex, lty = country)) + 
  geom_line(lwd = 1.1) +
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;) +
  ylab(&amp;quot;proportion&amp;quot;) + 
  ggtitle(&amp;quot;Proportion of babies that have one of the top 5% names \nAustralia and USA, 1960 -2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-20-01-babynames_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;names-are-getting-more-unique&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Names are getting more unique&lt;/h1&gt;
&lt;p&gt;Is the distribution of baby names become less concentrated because there are more unique names being used over time, or just because people are opting to choose less popular but already existing names?&lt;/p&gt;
&lt;p&gt;It seems that there is an increase in unique names being used over time in both countries. However, there has been a slight decrease in uniqueness in the US since 2010. (Perhaps people are finally running out of alternative ways of spelling ‘Jackson’.) Interestingly, the number of unique girls names is higher as a proportion of total births compared to boys. This is consistent with the observation above that Gini coefficients are higher for boys.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db %&amp;gt;% 
  group_by(sex, year, country) %&amp;gt;% 
  summarise(prop_uniq = n()/sum(count)) %&amp;gt;% 
  ggplot(aes(year, prop_uniq, color = sex, lty = country)) + 
  geom_line(lwd = 1.1) +
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;) +
  ylab(&amp;quot;proportion&amp;quot;) + 
  ggtitle(&amp;quot;Unique baby names as a proportion of total births \nAustralia and USA, 1960 -2015&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-20-01-babynames_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-notes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary notes&lt;/h1&gt;
&lt;p&gt;People tend to choose a baby name from a relatively small subset of popular names, although name uniqueness is increasing slightly over time. Concentration of baby names is generally higher for boys, and higher in the US compared to Australia. So even though there are many more interesting sounding names in the US, a larger proportion of the population just stick to the more usual names.&lt;/p&gt;
&lt;p&gt;Changes in popular baby names and how people choose to name their baby are influenced by underlying social processes, such as era-specific events, country-specific cultural norms, and fertility intentions. Sociologists and demographers such as &lt;a href=&#34;https://www.theatlantic.com/sexes/archive/2012/12/why-dont-parents-name-their-daughters-mary-anymore/265881/&#34;&gt;Phillip Cohen&lt;/a&gt; and &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0003122415621910&#34;&gt;Josh Goldstein&lt;/a&gt; have done some interesting work in this area.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For those who are a bit rusty on Australian geography, it’s a shame we don’t have Victoria and Queensland in particular, the two other big states.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that I chose the top 5% rather than the top 5 because of the large difference in the number of unique names across the two countries.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lifespan variation as a measure of mortality progress and inequality</title>
      <link>/2018/12/21/lifespan-variation-as-a-measure-of-mortality-progress-and-inequality/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/21/lifespan-variation-as-a-measure-of-mortality-progress-and-inequality/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post looks at how variation in lifespan has evolved over time for different states in the US, and how this measure complements trends in life expectancy. I was inspired to write this after hearing a great talk by Alyson van Raalte last week at MPIDR and reading her latest &lt;a href=&#34;http://science.sciencemag.org/content/362/6418/1002&#34;&gt;paper&lt;/a&gt; on the topic.&lt;/p&gt;
&lt;p&gt;One of the most common aggregate measure of mortality we tend to look at is life expectancy. The formal definition of (period) life expectancy at birth is the average number of years someone would live if the current age-specific mortality rates did not change in future. Given that age-specific mortality rates are generally improving over time, this isn’t really a realistic measure of longevity&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, but it’s a useful way of summarizing mortality at all ages into one number.&lt;/p&gt;
&lt;p&gt;The ‘expectancy’ part of the name comes from the fact that life expectancy is an average, or expectation. Of all the deaths that happen in a population, we are just looking at the average age at which they occur. However, there are many other ways of summarizing distributions with one number: for example, the median or mode age at death. Another such measure — lifespan variation — captures the variation in ages at death.&lt;/p&gt;
&lt;p&gt;As an example, the figure below shows the distribution of ages at deaths for Californian males in 1960 and 2010. (These data are from the &lt;a href=&#34;https://usa.mortality.org/&#34;&gt;HMD US states project&lt;/a&gt;). Notice that the distribution has shifted to the right, which corresponds to improving mortality and increased life expectancy (as shown by the vertical lines). In addition, notice that the distribution in 2010 is not as spread out — the distribution of deaths is more concentrated around the mean age. That is, variation in lifespan has decreased from 1960 to 2010.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/dx_CA.png&#34; width=&#34;600&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Increases in life expectancy are usually coupled with decreases in lifespan variation. This is due to the fact that life expectancy increases usually occur because of improvements in infant mortality (which leads to a decrease in the spike observed in the first-year mortality) and decreases in premature deaths (e.g. decreases in cardiovascular diseases). However, as van Raalte and coauthors point out, this relationship of increasing life expectancy and decreasing lifespan variation is not always the case, and recently there has been a reversal of the trend for many populations.&lt;/p&gt;
&lt;p&gt;I’ll briefly describe one way to calculate lifespan variation and show trends by US state. Note that all data come from the &lt;a href=&#34;https://usa.mortality.org/&#34;&gt;HMD US states Project&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-lifespan-variation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measuring lifespan variation&lt;/h2&gt;
&lt;p&gt;There are several different ways of measuring lifespan variation. In this post, I use the standard deviation of age of death, which can be calculated from lifetable quantities as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{\sum_0^\omega \frac{\left(x - e_0 \right)^2d_x}{\sum_0^\omega d_x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(e_0\)&lt;/span&gt; is life expectancy at birth, &lt;span class=&#34;math inline&#34;&gt;\(d_x\)&lt;/span&gt; is the lifetable deaths at age &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt; is the open age interval (110+ in the case of HMD data).&lt;/p&gt;
&lt;p&gt;There are many other options to calculate, including: standard deviation from age 10 (which eliminates the effects of child mortality, see &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1728-4457.2005.00092.x&#34;&gt;Edwards and Tuljapurkar&lt;/a&gt;); interquartile range; life disparity (&lt;span class=&#34;math inline&#34;&gt;\(e^\dagger\)&lt;/span&gt;) (e.g. see the new paper by &lt;a href=&#34;https://link.springer.com/article/10.1007/s13524-018-0729-9?wt_mc=alerts.TOCjournals&#34;&gt;Aburto and van Raalte&lt;/a&gt;). I chose standard deviation because I find it intuitive and it’s easy to calculate based on lifetable columns. However, other measures would probably show similar trends.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lifespan-variation-in-the-us&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lifespan variation in the US&lt;/h2&gt;
&lt;p&gt;The plot below shows life expectancy at birth and lifespan variation for the United States (data from &lt;a href=&#34;https://www.mortality.org&#34;&gt;HMD&lt;/a&gt;). The recent decline in life expectancy in the United States has gained a lot of &lt;a href=&#34;https://www.washingtonpost.com/national/health-science/us-life-expectancy-declines-again-a-dismal-trend-not-seen-since-world-war-i/2018/11/28/ae58bc8c-f28c-11e8-bc79-68604ed88993_story.html?noredirect=on&amp;amp;utm_term=.9fc59cc5fb08&#34;&gt;attention&lt;/a&gt;. However, note that lifespan variation started to increase before life expectancy started to plateau/decrease.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/USA.png&#34; width=&#34;600&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Looking by state, life expectancy has increased everywhere, with recent evidence of plateauing and declining in some states (note that these data are only up to 2015, so the declines would be more apparent with more recent data). On the other hand, lifespan variation has generally declined, but plateaued much earlier than life expectancy. An increase in lifespan variation is apparent for some states, including some New England and Mid-West states.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/facet.png&#34; width=&#34;5600&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;For example, the plots below show life expectancy and lifespan variation for New Hampshire and West Virginia, two states that have been &lt;a href=&#34;https://github.com/mkiang/opioid_hotspots&#34;&gt;hardest hit by the opioid epidemic&lt;/a&gt;. For these states, lifespan variation has increased back up to the level it was in the 1980s-90s.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/NH_WV.png&#34; width=&#34;800&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;It’s also interesting to compare trends in states that have a similar life expectancy. For instance, comparing Georgia and Ohio, the latter has experienced a much more pronounced increase in lifespan in recent years.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/GA_OH.png&#34; width=&#34;800&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Lifespan variation is easily calculable from lifetable quantities, and is an interesting measure of mortality progress and inequality. Even if life expectancy is increasing, the variation of lifespan could also be increasing, which suggests increased inequality in death – while a proportion of the population are dying at older ages, there is also an increased proportion dying prematurely. Increased variation in the age of death means greater uncertainty around timing of death, which has implications for how people think about their future.&lt;/p&gt;
&lt;p&gt;All data are freely available and the code I used to generate plots etc is available on my &lt;a href=&#34;https://github.com/MJAlexander/states-mortality&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/les_ja&#34;&gt;Leslie Root&lt;/a&gt; on Twitter for amusing rants on people misinterpreting life expectancies.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Trends in pregnancy-associated mortality involving opioids in the United States, 2007–2016</title>
      <link>/publication/opioid_maternal/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/opioid_maternal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Estimating age-specific mortality rates at the subnational level</title>
      <link>/2018/09/21/estimating-age-specific-mortality-rates-at-the-subnational-level/</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/21/estimating-age-specific-mortality-rates-at-the-subnational-level/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This is a tutorial on estimating age-specific mortality rates at the subnational level, using a model similar to that described in our &lt;a href=&#34;https://link.springer.com/article/10.1007/s13524-017-0618-7&#34;&gt;Demography paper&lt;/a&gt;. There are four main steps, which will be described below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Prepare data and get it in the right format&lt;/li&gt;
&lt;li&gt;Choose and create a mortality standard&lt;/li&gt;
&lt;li&gt;Fit the model&lt;/li&gt;
&lt;li&gt;Analyze results from the model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A few notes on this particular example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I’ll be fitting the model to county-level mortality rates in California over the years 1999 to 2016. These are age-specific mortality rates for both sexes for the age groups &amp;lt;1, 1-4, 5-9, 10-14, 15-19, 20-24, 25-34, 35-44, 45-54, 55-64, 65-74, 75-84, 85+.&lt;/li&gt;
&lt;li&gt;Data on deaths and populations are publicly available through &lt;a href=&#34;https://wonder.cdc.gov/&#34;&gt;CDC WONDER&lt;/a&gt;. However, age groups where death counts are less than 10 are suppressed, and so for some age group/year/county combinations, there are missing data. Also note that there are no observations for any years for two counties, Sierra and Alpine.&lt;/li&gt;
&lt;li&gt;All analysis was done in R and the model was fit using JAGS. Other MCMC options such as Stan, WinBUGS or PyMC would probably work just as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the code to reproduce this example can be found here: &lt;a href=&#34;https://github.com/MJAlexander/states-mortality/tree/master/CA_county_example&#34; class=&#34;uri&#34;&gt;https://github.com/MJAlexander/states-mortality/tree/master/CA_county_example&lt;/a&gt;. Please see the R file &lt;code&gt;CA.R&lt;/code&gt; in the &lt;code&gt;code&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;A note on modeling: there are many adaptions that can be made to this broad model set up, which may be more suitable in different situations. When estimating mortality in your own work, make sure to undergo a suitable validation process to see that the estimates are sensible, and fully test alternatives.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Preparing the data&lt;/h1&gt;
&lt;p&gt;The first step is to obtain data on death counts and population by age (and potentially sex) groups, and get it in the right format for modeling purposes. Note that you need counts, not just the mortality rates, as inputs into the model.&lt;/p&gt;
&lt;p&gt;In this example, I downloaded data on death and population counts by county (the files &lt;code&gt;CA.csv&lt;/code&gt; and &lt;code&gt;CA_pop.csv&lt;/code&gt; in the data folder). Because these two data sources had different age groups available, I had to a bit of cleaning up to make sure everything was consistent. The resulting deaths data has the following form:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##               county code age_group year deaths   pop age          mx
## 1 Alameda County, CA 6001  &amp;lt; 1 year 1999    110 19336   0 0.005688871
## 2 Alameda County, CA 6001  &amp;lt; 1 year 2000    104 19397   0 0.005361654
## 3 Alameda County, CA 6001  &amp;lt; 1 year 2001    133 22044   0 0.006033388
## 4 Alameda County, CA 6001  &amp;lt; 1 year 2002     91 21316   0 0.004269094
## 5 Alameda County, CA 6001  &amp;lt; 1 year 2003     97 21091   0 0.004599118
## 6 Alameda County, CA 6001  &amp;lt; 1 year 2004    111 20339   0 0.005457495&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the JAGS model, the data has to has to be in the form of an array. The notation used throughout the JAGS model is referring to age &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, area &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and state &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. So both the deaths and population data need to be in the form of an array with dimensions age x time x area x state. I did this in quite an ugly way combining loops and tidyverse, which probably isn’t the most elegant way, but it works :) The resulting deaths data for the first county (Alameda) looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y.xtas[,,1,1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
##  [1,]  110  104  133   91   97  111  105   91  105    89    77    94    79
##  [2,]   17   22   15   13   13   16   10   21   18    15    18    12    12
##  [3,]   14   14   16   15   10   13   NA   15   11    NA    NA    NA    NA
##  [4,]   16   19   17   13   19   13   22   13   NA    15    13    11    11
##  [5,]   48   36   41   57   51   50   51   66   54    47    43    51    34
##  [6,]   71   71   83   76   89   75   72   94   99    89    89    73    69
##  [7,]  194  193  179  195  187  201  171  194  167   169   156   120   173
##  [8,]  426  435  401  396  419  345  343  337  329   310   292   253   262
##  [9,]  779  799  783  836  827  825  812  758  768   735   708   629   677
## [10,] 1078 1069 1009 1066 1130 1064 1084 1106 1211  1176  1141  1159  1319
## [11,] 1779 1645 1531 1517 1445 1452 1362 1385 1368  1341  1333  1348  1353
## [12,] 2748 2792 2841 2714 2719 2517 2512 2395 2316  2271  2103  2088  2137
## [13,] 2629 2687 2720 2617 2748 2586 2782 2831 2874  2974  2922  3064  3041
##       [,14] [,15] [,16] [,17] [,18]
##  [1,]    75    86    76    75    71
##  [2,]    15    10    11    NA    10
##  [3,]    NA    NA    NA    NA    NA
##  [4,]    NA    12    NA    NA    NA
##  [5,]    49    43    38    46    37
##  [6,]    84    74    87    82    77
##  [7,]   163   159   180   178   214
##  [8,]   263   260   261   279   273
##  [9,]   675   647   549   598   603
## [10,]  1283  1273  1301  1291  1274
## [11,]  1528  1599  1624  1698  1750
## [12,]  2019  2040  2015  2059  2137
## [13,]  3253  3376  3276  3476  3462&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-the-mortality-standard&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Preparing the mortality standard&lt;/h1&gt;
&lt;p&gt;The other main inputs to the mortality model are the principal components derived from the mortality standard. Which mortality standard you choose to derive your principal components from depends on your specific problem. In the case of this example, I decided to use state-level mortality schedules for all states in the US over the period 1959–2015. These data are available through the &lt;a href=&#34;https://usa.mortality.org/&#34;&gt;United States Mortality Database&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code I used to create the principal components using these data are &lt;a href=&#34;https://github.com/MJAlexander/states-mortality/blob/master/CA_county_example/code/pcs.R&#34;&gt;here&lt;/a&gt;. Again note that for this particular example, I had to alter the data so that the age groups were consistent.&lt;/p&gt;
&lt;p&gt;Once the principal components are obtained, they can be input into the model based on being in a matrix with dimension age x component. Note that the model fitted here uses three components. The inputs are below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##            V1           V2           V3
## 1  0.20853194  0.630214628  0.287652535
## 2  0.35264710  0.300256735 -0.162737410
## 3  0.38660035  0.252316283 -0.287091193
## 4  0.38118755 -0.017848623 -0.403974070
## 5  0.32970586 -0.252356450 -0.352528704
## 6  0.31523511 -0.359905277 -0.025726734
## 7  0.30949609 -0.383953991  0.306658736
## 8  0.28246520 -0.191162991  0.454141280
## 9  0.24350004 -0.003865125  0.401059537
## 10 0.20458378  0.099035872  0.221135219
## 11 0.16646980  0.139019421  0.085803340
## 12 0.12715285  0.116507026  0.052966450
## 13 0.09332889 -0.169677526 -0.004089363&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Running the model&lt;/h1&gt;
&lt;p&gt;Now that we have the required data inputs, the JAGS model can be run. You need to create an input list of all the data required by JAGS, and specify the names of the parameters you would like to monitor and get posterior samples for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;jags.data &amp;lt;- list(y.xtas = y.xtas, 
                  pop.xtas = pop.xtas, 
                  Yx = pcs,
                  S = 1, X= length(age_groups), T = length(years), 
                  n.a=length(counties), n.amax=length(counties), P=3 )

parnames &amp;lt;- c(&amp;quot;beta.tas&amp;quot;, &amp;quot;mu.beta&amp;quot; ,&amp;quot;sigma.beta&amp;quot;, &amp;quot;tau.mu&amp;quot;, &amp;quot;u.xtas&amp;quot;, &amp;quot;mx.xtas&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once that is done, the model can be run. Please look at the model text file in reference to the paper to see which variables refer to what aspects. The notation used in the JAGS model is (I hope) fairly consistent with the notation in the paper.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- jags(data = jags.data, 
            parameters.to.save=parnames, 
            n.iter = 30000,
            model.file = &amp;quot;../code/model.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This may take a while to run, so be patient. You can look at a summary of the model estimates like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod$BUGSoutput$summary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the values of all Rhats should be less than 1.1, otherwise the estimates are unreliable and should not be interpreted. If you have Rhats that are greater than 1.1, try running the model for more iterations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check all Rhats are less than 1.1
max(mod$BUGSoutput$summary[,&amp;quot;Rhat&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Extract results&lt;/h1&gt;
&lt;p&gt;Now that we have model estimates, we need to be able to extract them and look at the results. You can get the posterior samples for all parameters by extracting the &lt;code&gt;sims.array&lt;/code&gt; from the model object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc.array &amp;lt;- mod$BUGSoutput$sims.array&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unless you’re interested in the underlying mechanics of the model, you’re probably most interested in the estimates for the age-specific mortality rates, &lt;code&gt;mx.xtas&lt;/code&gt;. The &lt;code&gt;sims.array&lt;/code&gt; has dimensions number iterations (default 1,000) x number of chains (default 3) x number of parameters. So to look at the posterior samples for &lt;code&gt;mx.xtas[1,1,1,1]&lt;/code&gt; for example, you would type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc.array[,,&amp;quot;mx.xtas[1,1,1,1]&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the posterior samples are obtained, these are used to obtain the best estimate of the parameter (usually the median) and Bayesian credible intervals. For example, a 95% credible interval can be calculated by getting the 2.5th and 97.5th quantile of the posterior samples. Below is a chart that illustrates some of the age-specific mortality estimates for six Californian counties in 2016. Code to generate this chart is included in &lt;code&gt;CA.R&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/select_counties_mx.png&#34; width=&#34;800&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Once the estimate for mortality rates are extracted, you can also convert these into other mortality measures, such as life expectancy, using standard life table relationships. The code on GitHub includes a function which derives life expectancy from the mx’s, called &lt;code&gt;derive_ex_values&lt;/code&gt;. This function is loaded in at the beginning of the &lt;code&gt;CA.R&lt;/code&gt;. Code to generate this chart is included at the end of &lt;code&gt;CA.R&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/e0_alameda.png&#34; width=&#34;600&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;This document gives a brief introduction into the practicalities of fitting a Bayesian subnational mortality model in R using JAGS. There are many different layers to the model and assumptions associated with it, so it is recommended that the user of this code and model is familiar with &lt;a href=&#34;https://link.springer.com/article/10.1007/s13524-017-0618-7&#34;&gt;the paper&lt;/a&gt; and the assumptions outlined in it. Good luck! :)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Thoughts after finishing a Demography PhD</title>
      <link>/2018/06/04/thoughts-after-finishing-a-demography-phd/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/04/thoughts-after-finishing-a-demography-phd/</guid>
      <description>


&lt;p&gt;PhDs are hard. They are incredibly fulfilling, but mentally challenging and emotionally draining. You meet some amazing people, but also have to deal with some difficult people and difficult situations. During my time as a PhD student, a lot of things went better than I imagined, but I also made a fair few mistakes.&lt;/p&gt;
&lt;p&gt;The following are a few thoughts after my experience. They are based on being involved in the demographic research field — a relatively small and supportive academic community — but the comments are pretty general. I’m writing this not so much to offer advice, but more in the hope that others read it and see similarities with their own experience.&lt;/p&gt;
&lt;div id=&#34;good-mentors-make-a-huge-difference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Good mentors make a huge difference&lt;/strong&gt;&lt;/h2&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Guess **Hey, join our book project: &lt;a href=&#34;https://t.co/po9FQ1j6ZQ&#34;&gt;https://t.co/po9FQ1j6ZQ&lt;/a&gt; &lt;a href=&#34;https://t.co/XS51temnki&#34;&gt;pic.twitter.com/XS51temnki&lt;/a&gt;&lt;/p&gt;&amp;mdash; PHD Comics (@PHDcomics) &lt;a href=&#34;https://twitter.com/PHDcomics/status/1002535988248416256?ref_src=twsrc%5Etfw&#34;&gt;June 1, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;Do not underestimate the effect of mentorship on your PhD experience: perhaps more than anyone, mentors influence your learning trajectory, research output and self confidence.&lt;/p&gt;
&lt;p&gt;Mentors need not be constrained to those on your formal advisory panel. In my own experience, I had a great panel, but the most invaluable mentorship came from people who were not formally associated with my committee or university.&lt;/p&gt;
&lt;p&gt;Find mentors who you feel comfortable around: there is huge value in being able to email or talk to someone about your research without being worried about making mistakes or saying something stupid. Good mentors listen, know about your research, introduce you to people, and think of you when they see opportunities. They do exist :)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-peer-review-process-can-be-long&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;The peer review process can be LONG&lt;/strong&gt;&lt;/h2&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Most scientists regarded the new streamlined peer-review process as &amp;quot;quite an improvement&amp;quot; &lt;a href=&#34;http://t.co/XEBcp2VavD&#34;&gt;pic.twitter.com/XEBcp2VavD&lt;/a&gt;&lt;/p&gt;&amp;mdash; The Sociological Review (@TheSocReview) &lt;a href=&#34;https://twitter.com/TheSocReview/status/520318869819252736?ref_src=twsrc%5Etfw&#34;&gt;October 9, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;It’s well known that the peer review process can take a long time. Journal articles can take 6-12 months from the time of submission to publication. In hindsight, I didn’t really think about this in terms of what it meant for my own timeline.&lt;/p&gt;
&lt;p&gt;Having R&amp;amp;Rs or publications can be very valuable when you go on the job market. If you want to be at this stage, you need to think about submitting a year before you go on the market, i.e. two years before you finish your PhD. This is a huge lead time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;doing-a-few-extra-projects-is-good-but-dont-do-too-many&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Doing a few extra projects is good, but don’t do too many&lt;/strong&gt;&lt;/h2&gt;
&lt;center&gt;
&lt;img src=&#34;/img/distraction.jpg&#34;, width = 500&gt;
&lt;/center&gt;
&lt;p&gt;There will probably be opportunities to be involved in side projects in addition to your dissertation. Side projects are a great way of learning new skills, collaborating with new researchers, and getting a little bit of extra income. However, it’s easy for side projects to end up taking most of your time: deadlines are often more imminent, which means it’s easy to put off the dissertation work for another day. Try not to fall into this trap.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;submit-and-go-to-conferences&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Submit and go to conferences&lt;/strong&gt;&lt;/h2&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Flying to a conference, the grad student loses the ability to finish some last-minute work. &lt;a href=&#34;https://twitter.com/hashtag/flying?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#flying&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/work?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#work&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/TRexArms?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TRexArms&lt;/a&gt; &lt;a href=&#34;https://t.co/teyPbAVjt6&#34;&gt;pic.twitter.com/teyPbAVjt6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lego Grad Student (@legogradstudent) &lt;a href=&#34;https://twitter.com/legogradstudent/status/766336056539029504?ref_src=twsrc%5Etfw&#34;&gt;August 18, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;Conferences are a great way of getting your work out there. If you’re like me, they are also a great way of forcing yourself to have a deadline to get a working paper finished. Conferences are also a good opportunity to try and meet other researchers in your field. Introduce yourself to people who you might be interested in working with, so they can put a face to the name in future.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-little-bit-everyday&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Write a little bit everyday&lt;/strong&gt;&lt;/h2&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;und&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://t.co/rL9oCvRBr2&#34;&gt;pic.twitter.com/rL9oCvRBr2&lt;/a&gt;&lt;/p&gt;&amp;mdash; Shit Academics Say (@AcademicsSay) &lt;a href=&#34;https://twitter.com/AcademicsSay/status/996526193510899712?ref_src=twsrc%5Etfw&#34;&gt;May 15, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;Demographers often like to play with data and graph things instead of writing. Trying to write a little bit most days helps immensely later on. Sometimes Past Monica was surprisingly clever, sometimes Past Monica was talking rubbish, but Present Monica was generally glad Past Monica had put some thoughts to paper.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;try-not-to-compare-yourself-to-others-too-much&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Try not to compare yourself to others too much&lt;/strong&gt;&lt;/h2&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The continued popularity of both the &amp;quot;PhD students literally never stop working&amp;quot; genre and the &amp;quot;lol I wrote one sentence today, good job me&amp;quot; genre on the ol&amp;#39; TL is a great reminder not to take people&amp;#39;s assertions about their productivity at face value&lt;/p&gt;&amp;mdash; Leslie Root (@les_ja) &lt;a href=&#34;https://twitter.com/les_ja/status/999307291873574913?ref_src=twsrc%5Etfw&#34;&gt;May 23, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;It’s hard to not suffer from imposter syndrome. It’s hard to not compare yourself to others and their achievements. It’s hard not worry about whether you’re doing the right topic, selling your work enough, publishing in the right places. And you will likely come across really competitive people that want to put you down to make themselves feel better.&lt;/p&gt;
&lt;p&gt;But in the end, it’s important to focus on yourself: define your own research interests, know what your goals are and work to achieve them. Back yourself, and be proud of your work. And don’t worry about the passive aggressors: surround yourself with fun, supportive and interesting people.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;its-okay-to-feel-overwhelmed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;It’s okay to feel overwhelmed&lt;/strong&gt;&lt;/h2&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Gotta tape this up somewhere &lt;a href=&#34;https://t.co/5tCZBHtUuw&#34;&gt;pic.twitter.com/5tCZBHtUuw&lt;/a&gt;&lt;/p&gt;&amp;mdash; Emily Silverman (@ESilvermanMD) &lt;a href=&#34;https://twitter.com/ESilvermanMD/status/1002242560633573377?ref_src=twsrc%5Etfw&#34;&gt;May 31, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;Most people feel overwhelmed at some point, and it’s okay. It’s not a personal failing. Sometimes, it’s part of the learning process. You’re doing something you’ve never done before, at the limit of your knowledge, and there’s no guarantee it will work (and it often doesn’t). That said, &lt;a href=&#34;https://www.nature.com/articles/nbt.4089&#34;&gt;many graduate students suffer from mental health issues&lt;/a&gt; and it’s important to surround yourself with supportive people and seek help when you need it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Final thoughts &lt;/strong&gt;&lt;/h2&gt;
&lt;div id=&#34;good-research-is-important-but-so-is-working-hard-and-being-nice-to-people.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Good research is important, but so is working hard and being nice to people.&lt;/h3&gt;
&lt;p&gt;In the end, PhDs (and academia) are similar to any other job. Being successful is not just a function of how good your topic is. You have to put in the hours. You also have to learn to deal with people you disagree with in a professional manner. Be supportive of your peers and promote other people’s work, not just your own. This helps to build a productive and supportive network for the future.&lt;/p&gt;
&lt;p&gt;Around six years ago, &lt;a href=&#34;https://www.rohanalexander.com/&#34;&gt;Rohan&lt;/a&gt; convinced me to take the GRE and apply for some PhD programs. “If you don’t try, you’ll never know, and then you’ll always wonder if you could have.” This was right, of course. One year later we were on a plane, moving from Australia to the US.&lt;/p&gt;
&lt;p&gt;I graduated about three weeks ago and it still hasn’t really sunk in. I feel very lucky to have had this opportunity.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Trends in Black and White Opioid Mortality in the United States, 1979–2015</title>
      <link>/publication/opioid_trends/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/opioid_trends/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deaths without denominators: using a matched dataset to study mortality patterns in the United States</title>
      <link>/publication/censoc/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/censoc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Berkeley Demography at PAA 2018</title>
      <link>/2018/04/24/berkeley-demography-at-paa-2018/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/24/berkeley-demography-at-paa-2018/</guid>
      <description>


&lt;p&gt;Berkeley Demography and Population Center affiliates will be presenting their work at PAA 2018. Check out some of their great work!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person11604.html&#34;&gt;Magali Barbieri&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Session 6-2: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper20592.html&#34;&gt;Contribution of drug poisonings to divergence in life expectancy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Session 45-3: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23025.html&#34;&gt;Cause-specific mortality data at the subnational level&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person4688.html&#34;&gt;Boroka Bo&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Poster (Health and Mortality 1): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper21393.html&#34;&gt;Time poverty by ethnicity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person2557.html&#34;&gt;Gabriel Borges&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Session 68-4: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23921.html&#34;&gt;Mortality rates from sibling histories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Session 83-2: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper20223.html&#34;&gt;Bayesian melding to estimate census coverage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person11985.html&#34;&gt;Stephanie Child&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Poster (Health and Mortality 1): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper19876.html&#34;&gt;Social networks and stress&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person264.html&#34;&gt;Will Dow&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Session 72-3: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper20106.html&#34;&gt;Cuba’s cardiovascual risk factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Poster (Children and Youth): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper18486.html&#34;&gt;Parenting and Early Childhood Development in Indigenous and Non-Indigenous Mexican Communities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Poster (Health and Mortality 2): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper21504.html&#34;&gt;Incentive-Based Interventions for Smoking Cessation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Poster (Marriage, Families, Households, and Unions 2; Gender, Race, and Ethnicity): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23528.html&#34;&gt;Paid parental leave&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person13558.html&#34;&gt;Denys Dukhovnov&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Poster (Marriage, Families, Households, and Unions 1): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper21415.html&#34;&gt;Stress coping strategies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Poster (Health and Mortality 1): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper21393.html&#34;&gt;Time poverty by ethnicity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person14546.html&#34;&gt;Dennis Feehan&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Session 68-4: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23921.html&#34;&gt;Mortality rates from sibling histories&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Session 167-3: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23912.html&#34;&gt;Estimating internet adoption using Facebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Session2138.html&#34;&gt;Joshua Goldstein&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;Organizer, Mathematical demography session&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person255.html&#34;&gt;Ron Lee&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Session 95-2: &lt;a href=&#34;paa/2018/webprogrampreliminary/Paper21830.html&#34;&gt;Life expectancy, pension outcomes and income&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Session 140-3: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper20137.html&#34;&gt;Aging and intergenerational flows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person6435.html&#34;&gt;Hayley Pierce&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Session 160-4: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper18645.html&#34;&gt;Risk and Protective Factors for Generational Refugee Children&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person2766.html&#34;&gt;Danny Schneider&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Session 167-2: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23984.html&#34;&gt;Facebook as a Tool for Survey Data Collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Session 174-4: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper24054.html&#34;&gt;Inequalities in parental time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person9219.html&#34;&gt;Ruijie (Mia) Zhong&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Poster (Marriage, Families, Households, and Unions 2; Gender, Race, and Ethnicity): &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23830.html&#34;&gt;Education and marriage in Japan&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;…and me!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person6437.html&#34;&gt;Monica Alexander&lt;/a&gt;:
&lt;ul&gt;
&lt;li&gt;Session 89-2: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper22264.html&#34;&gt;Spatial distribution of opioid mortality&lt;/a&gt;, presented by &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person15557.html&#34;&gt;Mathew Kiang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Session 113-2: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper21488.html&#34;&gt;Estimating subnational populations&lt;/a&gt;, joint work with &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person16520.html&#34;&gt;Leontine Alkema&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Session 207-3: &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Paper23641.html&#34;&gt;Using Facebook and ACS to predict migration stocks&lt;/a&gt;, presented by &lt;a href=&#34;https://paa.confex.com/paa/2018/webprogrampreliminary/Person1727.html&#34;&gt;Emilio Zagheni&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;img src=&#34;/img/demogseal2.png&#34;, width = 200&gt;
&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Estimating Subnational Populations of Women of Reproductive Age</title>
      <link>/publication/kenya_subnational/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/kenya_subnational/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gompertz mortality models: the relationship between alpha, beta and mode age</title>
      <link>/2018/02/18/gompertz-mortality-models-the-relationship-between-alpha-beta-and-mode-age/</link>
      <pubDate>Sun, 18 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/18/gompertz-mortality-models-the-relationship-between-alpha-beta-and-mode-age/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The Gompertz model is one of the most well-known mortality models. It does remarkably well at explaining mortality rates at adult ages across a wide range of populations with just two parameters. This post briefly reviews the Gompertz model, highlighting the relationship between the two Gompertz parameters, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, and the implied mode age at death. I focus on the situation where we only observe death counts by age (rather than mortality rates), so estimation of the Gompertz model requires choosing &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to maximize the (log) density of deaths.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gompertz-mortality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gompertz mortality&lt;/h2&gt;
&lt;p&gt;Here are a few important equations related to the Gompertz model.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The Gompertz hazard (or force of mortality) at age &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mu(x)\)&lt;/span&gt;, has the exponential form &lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \alpha e^{\beta x}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; parameter captures some starting level of mortality and the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; gives the rate of mortality increase over age. Note here that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; refers to the starting age of analysis and not necessarily age = 0. Indeed, Gompertz models don’t do a very good job at younger ages (roughly &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;40\)&lt;/span&gt; years).&lt;/p&gt;
&lt;p&gt;Given the relationship between hazard rates and survivorship at age &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(l(x)\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = -\frac{d}{dx} \log l(x)
\]&lt;/span&gt; the expression for &lt;span class=&#34;math inline&#34;&gt;\(l(x)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
l(x) = \exp\left(-\frac{\alpha}{\beta}\left(\exp(\beta x) - 1\right)\right)
\]&lt;/span&gt; It then follows that the density of deaths at age &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d(x)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
d(x) = \mu(x) l(x) = \alpha \exp(\beta x) \exp\left(-\frac{\alpha}{\beta}\left(\exp(\beta x) - 1\right)\right)
\]&lt;/span&gt; which probably looks worse than it is. &lt;span class=&#34;math inline&#34;&gt;\(d(x)\)&lt;/span&gt; tells us about the distribution of deaths by age. It is a density, so &lt;span class=&#34;math display&#34;&gt;\[
\int d(x) = 1
\]&lt;/span&gt; Say we observe death counts by age, &lt;span class=&#34;math inline&#34;&gt;\(y(x)\)&lt;/span&gt;, which implies a total number of deaths of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;. If we multiply the total number of deaths &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(d(x)\)&lt;/span&gt;, then that gives the number of deaths at age &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. In terms of fitting a model, we want to find values for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that correspond to the density &lt;span class=&#34;math inline&#34;&gt;\(d(x)\)&lt;/span&gt; which best describes the data we observe, &lt;span class=&#34;math inline&#34;&gt;\(y(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameterization-in-terms-of-the-mode-age&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameterization in terms of the mode age&lt;/h2&gt;
&lt;p&gt;Under a Gompertz model, the mode age at death, &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
M = \frac{1}{\beta}\log \left(\frac{\beta}{\alpha}\right)
\]&lt;/span&gt; Given a set of plausible mode ages, we can work out the relevant combinations of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; based on the equation above. For example, the chart belows shows all combinations of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; that result in a mode age between 60 and 90.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/plausible_values.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart suggests that plausible values of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; for human populations are pretty restricted. In addition, it shows the strong correlation between these two parameters: in general, the smaller the value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, the larger the value of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. This sort of correlation between parameters can cause issues with estimation. However, given we know the relationship between &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and the mode age, the Gompertz model can be reparameterized in terms of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \beta \exp\left(\beta (x - M)\right)
\]&lt;/span&gt; As &lt;a href=&#34;https://www.demographic-research.org/volumes/vol32/36/&#34;&gt;this paper&lt;/a&gt; notes, &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; are much less correlated than &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. In addition, the modal age has a much more intuitive interpretation than &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implications-for-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implications for fitting&lt;/h2&gt;
&lt;p&gt;Given the reparameterization, we now want to find estimates for &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; such that the resulting deaths density &lt;span class=&#34;math inline&#34;&gt;\(d(x)\)&lt;/span&gt; best reflects the data. If we assume that the number of deaths observed at a particular age, &lt;span class=&#34;math inline&#34;&gt;\(y_x\)&lt;/span&gt;, are Poisson distributed, and the total number of deaths observed is &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;, then we get the following hierarchical set up:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y(x) \sim \text{Poisson} (\lambda(x))\\
\lambda(x) = D \cdot d(x)\\
d(x) = \mu(x) \cdot l(x)\\
\mu(x) = \beta \exp\left(\beta (x - M)\right) \\
l(x) = \exp \left( -\exp \left(-\beta M \right) \left(\exp(\beta x)-1 \right)\right)
\]&lt;/span&gt; This can be fit in a Bayesian framework, with relevant priors put on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;end-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;End notes&lt;/h3&gt;
&lt;p&gt;This is part of an ongoing project with &lt;a href=&#34;http://www.site.demog.berkeley.edu/josh-goldstein&#34;&gt;Josh Goldstein&lt;/a&gt; on modeling mortality rates for a dataset of censored death observations. Thanks to &lt;a href=&#34;http://www.robertempickett.com/&#34;&gt;Robert Pickett&lt;/a&gt; who told me about the Tissov et al. paper and generally has interesting things to say about demography.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A good reference for this is &lt;a href=&#34;http://www.hup.harvard.edu/catalog.php?isbn=9780674045576&#34;&gt;Essential Demographic Methods, Chapter 3&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Global Estimation of Neonatal Mortality using a Bayesian Hierarchical Splines Regression Model</title>
      <link>/publication/estimating_neonatal_mortality/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/estimating_neonatal_mortality/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using SVD in demographic modeling</title>
      <link>/2017/12/16/using-svd-in-demographic-modeling/</link>
      <pubDate>Sat, 16 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/16/using-svd-in-demographic-modeling/</guid>
      <description>


&lt;p&gt;A core objective of demographic modeling is finding empirical regularities in age patterns in fertility, mortality and migration. One method to achieve this goal is using Singular Value Decomposition (SVD) to extract characteristic age patterns in demographic indicators over time. This post describes how SVD can be used in demographic research, and in particular, mortality estimation.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;The SVD of matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
X = UDV^T
\]&lt;/span&gt; The three matrices resulting from the decomposition have special properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The columns of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; are orthonormal, i.e. they are orthogonal to each other and unit vectors. These are called the left and right singular vectors, respectively.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is a diagonal matrix with positive real entries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, the components obtained from SVD help to summarize some characteristics of the matrix that we are interested in, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. In particular, the first right singular vector (i.e. the first column of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;) gives the direction of the maximum variation of the data contained in &lt;span class=&#34;math inline&#34;&gt;\(X.\)&lt;/span&gt; The second right singular vector, which is orthogonal to the first, gives the direction of the second-most variation of the data, and so on. The &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; elements represent additional rotation and scaling transformations to get back the original data in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;SVD is useful as a dimensionality reduction technique: it allows us to describe our dataset using fewer dimensions than implied by the original data. For example, often a large majority of variation in the data is captured by the direction of the first singular vector, and so even just looking at this dimension can capture key patterns in the data. SVD is closely related to Principal Components Analysis: principal components are derived by projecting data &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; onto principal axes, which are the right singular vectors &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-in-demographic-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use in demographic modeling&lt;/h2&gt;
&lt;p&gt;Using SVD for demographic modeling and forecasting first gained popularity after &lt;a href=&#34;https://www.jstor.org/stable/2290201&#34;&gt;Lee and Carter&lt;/a&gt; used the technique as a basis for forecasting US mortality rates. They modeled age-specific mortality on the log scale as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\log m_x = a_x + b_x \cdot k_t
\]&lt;/span&gt; where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a_x\)&lt;/span&gt; is the mean age-specific mortality schedule across all years of analysis,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_x\)&lt;/span&gt; is the average contribution of age group &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to overall mortality change over the period, and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(k_t\)&lt;/span&gt; is the incremental change in period &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The latter two quantities are obtained via SVD of a time x age matrix of demeaned, logged mortality rates: &lt;span class=&#34;math inline&#34;&gt;\(b_x\)&lt;/span&gt; is the first right singular vector, while &lt;span class=&#34;math inline&#34;&gt;\(k_t\)&lt;/span&gt; is the first left singular vector multiplied the first element of &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;More recently, SVD has become increasingly used in demographic modeling; for example &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/01621459.2014.881738&#34;&gt;Carl Schmertmann et al.&lt;/a&gt; used it to model and forecast cohort fertility, &lt;a href=&#34;https://arxiv.org/abs/1612.01408&#34;&gt;Sam Clark&lt;/a&gt; to estimate age schedules of mortality with limited data, and &lt;a href=&#34;https://link.springer.com/article/10.1007/s13524-017-0618-7&#34;&gt;Emilio Zagheni, Magali Barbieri and myself&lt;/a&gt; to model subnational age-specific mortality.&lt;/p&gt;
&lt;div id=&#34;example-age-specific-mortality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example: age-specific mortality&lt;/h3&gt;
&lt;p&gt;Imagine you have observations of age-specific mortality rates in multiple years. Create a matrix, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, where each row represents the age-specific mortality rates in a particular year. Modeling of mortality rates is often done on the log scale (to ensure rates are positive), so you may want to take the log of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Then do a SVD on this matrix - in &lt;code&gt;R&lt;/code&gt; this is as easy as &lt;code&gt;svd(x)&lt;/code&gt;. The age patterns of interest are then contained in the resulting &lt;code&gt;v&lt;/code&gt; matrix; so for example &lt;code&gt;svd(x)$v[,1:3]&lt;/code&gt; would give you the first three age ‘principal components’ of your matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/svd_plots/svd.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For example, the first three principal components of US male mortality by state over the years 1980-2010 are plotted below. Each component has a demographic interpretation - the first represents baseline mortality, the second represents higher-than-baseline child mortality, and the third represents higher-than-baseline adult mortality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/svd_plots/3pcs_states_neg.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;For modeling, the idea is that different linear combinations of these components allow you to flexibly represent a wide range of different mortality curves. For example, log-mortality rates could be modeled as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\log m_x = \beta_1 Y_{1x} + \beta_2 Y_{2x} + \beta_3 Y_{3x}
\]&lt;/span&gt; where the &lt;span class=&#34;math inline&#34;&gt;\(Y_{.x}\)&lt;/span&gt;’s are the principal components above and the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;’s are to be estimated. The plot below shows four different mortality curves derived from the US male principal components with different coefficient settings. You can also play with different settings interactively &lt;a href=&#34;http://shiny.demog.berkeley.edu/monicah/mort/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/svd_plots/coeff.png&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-race-specific-opioid-mortality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example: race-specific opioid mortality&lt;/h3&gt;
&lt;p&gt;This technique of representing and modeling underlying age patterns need not be restricted to modeling all-cause mortality. For example, SVD proves useful when looking at deaths due to opioid overdoses by race and state in the US. Even though &lt;a href=&#34;https://www.monicaalexander.com/2017/05/02/opioid-mortality-by-race-from-divergence-to-convergence/&#34;&gt;opioid overdoses are rapidly increasing for both the black and white population&lt;/a&gt;, overdoses are still a relatively rare event, and so death rates calculated from the raw data suffer from large stochastic/random variation.&lt;/p&gt;
&lt;p&gt;For example, the chart below shows age-specific opioid mortality rates by race for North Carolina in 2004.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; As you can see, for the black population there are quite a few age groups were there are zero observed deaths, so the observed mortality rate is zero. However, given what we know about how mortality evolves over age, the zero observed death rates are likely due to random variation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/svd_plots/NC_age.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Even though age patterns are noisy at the state level, we have an idea of age patterns by race in opioid mortality in the national level. So we can use these national age patterns - via information captured in a SVD - to help model underlying mortality rates at the state level.&lt;/p&gt;
&lt;p&gt;The figure below shows the first two principal components derived using SVD from race-specific opioid mortality in the US over the years 1999-2015. The first principal component again represents a baseline mortality schedule for opioid-related deaths for each race. The second principal component represents the contribution of each age group to mortality change over time. Notice the ‘double-humped’ shape for the white population - this is driven by heroin deaths being concentrated at younger ages, and prescription opioid-related deaths being concentrated at older ages.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/svd_plots/opioid_pcs.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Similar to the example above, we can use these principal components as a basis of a regression framework to estimate underlying age-specific mortality rates by age. Results from such a model for North Carolina in 2004 are shown below. The dots represent mortality rates calculated from the raw data, as above. The lines and associated shaded area represent estimates of the underlying mortality rates with 95% uncertainty intervals. These were obtained from a model that utilized information from the principal components. Instead of dealing with zero observed deaths, we now have estimates that give more plausible values for the underlying mortality rates.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/svd_plots/NC_agefit.png&#34;&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;SVD is a useful technique to extract the main characteristics of age patterns in demographic indictors. These structural age patterns are useful to get a better idea of underlying processes when available data are sparse or noisy. Age patterns derived from SVD can be flexibly shifted and adjusted based on available data. Built-in functions in &lt;code&gt;R&lt;/code&gt; make it relatively easy to use SVD to better understand, model and project demographic indicators.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;cross-promotional plug: you can play with this data yourself with the help of the &lt;a href=&#34;https://github.com/mkiang/narcan&#34;&gt;narcan&lt;/a&gt; &lt;code&gt;R&lt;/code&gt; package, which &lt;a href=&#34;http://mathewkiang.com&#34;&gt;Mathew Kiang&lt;/a&gt; and I are working on.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Comparing methods for smoothing temporal demographic data, and the distortr package</title>
      <link>/2017/10/31/comparing-methods-for-smoothing-temporal-demographic-data-and-the-distortr-package/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/31/comparing-methods-for-smoothing-temporal-demographic-data-and-the-distortr-package/</guid>
      <description>


&lt;p&gt;At the International Population Conference of the International Union for the Scientific Study of Population (IUSSP) I will present work on comparing different methods for smoothing demographic data. This post briefly outlines the motivation for the project and describes the R package &lt;code&gt;distortr&lt;/code&gt; which accompanies the project.&lt;/p&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;An important part of demographic research is the ability to estimate and project time series of demographic and health indicators. However, it is often the case that populations that have the poorest outcomes also have poor-quality data. In these cases, the underlying trends may be unclear due to missing data or overly messy data.&lt;/p&gt;
&lt;p&gt;In such situations, demographers often employ statistical models to help estimate and understand underlying trends. Often, these statistical models have the general form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta_t = f(X_t) + Z_t + \varepsilon_t
\]&lt;/span&gt; where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_t\)&lt;/span&gt; is the outcome of interest (mortality rate, fertility rate, etc)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(X_t)\)&lt;/span&gt; is a regression framework, a function of covariates &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; are temporal distortions, which capture data-driven non-linear trends over time, not otherwise captured in &lt;span class=&#34;math inline&#34;&gt;\(f(X_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_t\)&lt;/span&gt; is an error term.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the inclusion of covariates in the regression framework is often well justified, the choice for modeling the distortions &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; is often more arbitrary. However, models for &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; are important: they allow for data-driven trends that may not be captured by simple regression models; they smooth distortions, accounting for error in data observations; they incorporate uncertainty in the underlying processes; and allow for a temporal mechanism to be projected into the future. Different model choice can sometimes lead to vastly different estimates.&lt;/p&gt;
&lt;p&gt;This project aims to compare three main families of temporal models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ARMA models&lt;/li&gt;
&lt;li&gt;Gaussian Process regression&lt;/li&gt;
&lt;li&gt;Penalized splines regression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The aim is to compare the three methods theoretically and see how differences manifest into differences in estimates for different data scenarios.&lt;/p&gt;
&lt;p&gt;The paper presented at IUSSP is available &lt;a href=&#34;https://www.monicaalexander.com/pdf/temporal_smoothing.pdf&#34;&gt;here&lt;/a&gt;, and the slides are &lt;a href=&#34;https://github.com/MJAlexander/distortr/blob/master/IUSSP_011117.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-distortr-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;distortr&lt;/code&gt; package&lt;/h2&gt;
&lt;p&gt;As part of this project, I am developing an R package to aid in comparing and fitting different models for estimation of demographic time series. The &lt;code&gt;distortr&lt;/code&gt; package is available on &lt;a href=&#34;https://github.com/MJAlexander/distortr&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The package consists of two main parts:&lt;/p&gt;
&lt;div id=&#34;simulate-time-series-of-distortions-and-fit-and-validate-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Simulate time series of distortions, and fit and validate models&lt;/h3&gt;
&lt;p&gt;This part of the package contains tools to investigate how different models perform in different simulation settings and how much it matters if the ‘wrong’ model is chosen.&lt;/p&gt;
&lt;p&gt;Simulated time series of data can be created from any of the following processes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AR(1)&lt;/li&gt;
&lt;li&gt;ARMA(1,1)&lt;/li&gt;
&lt;li&gt;P-splines (first or second order penalization)&lt;/li&gt;
&lt;li&gt;Gaussian Process (squared exponential or Matern function)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The various parameters associated with each function can be specified. The user can also specify how much of the time series is missing, and the sampling error around data. The sample autocorrelation function of the time series can also be plotted.&lt;/p&gt;
&lt;p&gt;In terms of model fitting, any of the above models can be fit to simulated data. Projections of time series can also easily be obtained. Estimates and uncertainty around estimates can be outputted and plotted.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-bayesian-hierarchical-models-to-datasets-with-observations-from-multiple-areas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Fit Bayesian hierarchical models to datasets with observations from multiple areas&lt;/h3&gt;
&lt;p&gt;Given data are often sparse or unreliable, especially in the case of developing countries, models that estimate demographic indicators for multiple areas/countries are often hierarchical, incorporating pooling of information across geographies. This part of the package has the infrastructure to fit Bayesian hierarchical models using one of the temporal smoothing methods. The user can specify whether or not to include a linear trend, and the type of temporal smoother to fit to the data.&lt;/p&gt;
&lt;p&gt;Datasets with observations from multiple countries/areas can be used, with the following columns required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;country/area name or code (e.g. country ISO code)&lt;/li&gt;
&lt;li&gt;value of observation&lt;/li&gt;
&lt;li&gt;year of observation&lt;/li&gt;
&lt;li&gt;sampling error of observation&lt;/li&gt;
&lt;li&gt;data source (e.g. survey, administrative)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, a region column may also be included (e.g. World Bank region). By default the built-in models include a region hierarchy (i.e. a country within a region within the world). However, models can also be run without the region level.&lt;/p&gt;
&lt;p&gt;For an example using real data, refer to the file &lt;a href=&#34;https://github.com/MJAlexander/distortr/blob/master/real_data_anc4_example.R&#34;&gt;real_data_anc4_example.R&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;One of the aims of this project was to provide the tools to increase transparency of model choice and to help demographers and policymakers understand differences in models and sensitivities of estimates to model choice. The &lt;code&gt;distortr&lt;/code&gt; package provides some infrastructure to explore and fit different methods. It is a work in progress and any feedback is much appreciated.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Flexible Bayesian Model for Estimating Subnational Mortality</title>
      <link>/publication/a_flexible_bayesian_model/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/a_flexible_bayesian_model/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
